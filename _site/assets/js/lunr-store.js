var store = [ 
    
    
    { 
        "url": "/vra_2025/content/1_intro.html",
        "title": "Introduction",
        "text": "My name is Andrew Weymouth and I am the Digital Initiatives Librarian for University of Idaho. Today I’ll be discussing two practical applications implementing computer vision that can facilitate batch processing large numbers of digital visual resources. &#10042; The team I’m a part of in the Center for Digital Inquiry and Learning consists of three other people and a small group of student workers, collaborating with the five members of our Special Collections department to create and maintain the 150 digital collections on our main institutional repository as well as the many other, more customized fellowship projects. I also develop digital tools and workflows to enhance transcription, optical character recognition for archival documents, tagging, and image processing, in order to make the university’s audio, text, and visual resources more discoverable for researchers. The U of I Digital Collections, moving from the main browse page to a collection and down to the item level Both of the tools I’ll be speaking about today have their origins in finding practical solutions to repetitive, digital work that comes after the hands-on arranging, research, cataloguing and description work in the archive. Considering I work with technology quite a bit, I would broadly self identify as critical towards AI applications to cultural heritage work, which I think you’ll find reflective in the coding approach, where sets of algorithms and neural networks are implemented on guardrails within a programmatic workflow, to make sure results are scalable and transparent. &#10042; Definitions It might be helpful to begin with some definitions of what resources these tools implement. Both tools are written in Python, a general purpose, readable programming language, and they are hosted in GitHub, a static web platform for hosting software development projects which allows these tools to be implemented in controlled environments and avoid software versioning complications. Representation of algorithmic shared characteristics. A large language model is necessarily a neural network but a neural network is not necessarily a large language model, a neural network is necessarily machine learning but machine learning... and so on. The first tool I’ll discuss implements an artificial intelligence resource. At the moment, most of us would associate AI with large language models that are trained on text and reproduce human language styled output. In contrast, this tool is a more general neural network, computer architecture in which processors are interconnected in a manner suggestive of the human brain, whose outputs are programmatic rather than imitating human speech. The second tool utilizes machine learning which is a computational method that enables a computer to learn to perform tasks by analyzing a large dataset without being explicitly programmed, in that case to focus on pattern recognition. All of the other resources these tools utilize are Python libraries, wrappers that distribute any of the above models as well as path utilities, which are untrained, deterministic functions. &#10042;"
    },
    { 
        "url": "/vra_2025/content/2_ed.html",
        "title": "edge_detector tool",
        "text": "The first tool was suggested by Evan Williamson, Digital Infrastructure Librarian at the U of I, prompted by the large number of archeological projects that we were collaborating on, including this example of an archeological excavation at the local Moscow High School which unearthed this paleolithic bug juice container and He-Man leg. Examples of archeological context photos where we needed to extract the object from it's background All of these projects were controlled context photographs of the object beside a ruler, possibly a color swatch, with varying backgrounds and lighting setups. The idea was to identify and extract the objects and, since different fellowship collaborators wanted these objects to be reproduced with both white and black backgrounds, we thought providing both of these options as well as a PNG file with a transparent background would be optimal. &#10042; edge_detector tool code snippet from pathlib import Path: handling directory and file paths from rembg import remove, new_session: third party library accessing ISNET computer vision from PIL import Image: pillow library for opening, editing and saving project input_folder = Path(‘A’): original image file containing object that needs extracting transparent_folder = Path(‘B’): extracted object with transparent background (PNG) white_folder = Path(‘C’): white background (JPG) black_folder = Path(‘D’): black background (JPG) for folder in [transparent_folder, white_folder, black_folder] folder.mkdir(exist_ok=True): checking that folders exist before processing session = new_session(model_name=”isnet-general-use”): engage in new session with model for img_path in input_folder.glob: if not img_path.suffix.lower() in [‘.jpg’, ‘.jpeg’, ‘.png’, ‘.tiff’]: continue: skip unsupported image file types in A folder input_image = Image.open(img_path).convert(“RGB”): load image and convert to RGB … Find the full script here_ The tool implements a neural network called IS-Net, originally developed for a paper titled “Highly Accurate Dichotomous Image Segmentation” by Xuebin Qin and others in 2022. The model executes fine grain, binary foreground/background segmentation and was trained on/for isolating retail objects for online marketplaces. The model is completely open access, requiring the user to open and close sessions within the Python script to use, but not requiring a monetary API key. &#10042; U2Net vs. isnet-general-use model computer vision There are a few iterations that can be implemented under the greater rembg (remove background) library. At first, I was using the original u2net model, but found that the isnet-general-use model is more accurate and produces finer lines around the object, if a little slower to process. &#10042;"
    },
    { 
        "url": "/vra_2025/content/3_ie.html",
        "title": "image_extractor tool",
        "text": "Page from the Tacoma Mountaineers 1911 scrapbook, courtesy Tacoma Northwest Room. The second tool was prompted by an archivist digitizing scrapbooks with our Zeutschel overhead scanner and wondering if we could batch extract individual photographs from the full page images. The image processing here is done using Scikit-Image, a collection of algorithms that can be implemented in different ways depending on the type of content you need to mask and extract. &#10042; image_extractor tool code snippet from skimage import io, color, filters, measure, morphology, util: image reading, gray scale conversion for preprocessing, region analysis and labeling from skimage.morphology import binary_closing, remove_small_objects: “closing” object edges and removing noise from scipy.ndimage import binary_fill_holes: “fill” interior holes in identified objects import numpy as np: replace Python loops with array operations for speed import os: file and directory operations and path manipulation from pathlib import Path: cleaning syntax for path manipulations import time: performance logging from PIL import Image: converting to final image format Parameters: min_long_edge = 300 max_long_edge = 1000 max_photos = 10max_photos … Find the full script here The design of this code is a little different than the previous tool, with simple, human readable parameters you can adjust depending on the general size of the photos in their collection, the number of photos within an image and the amount of margin around those images. The code identifies each photograph within the image placed in the A folder and generates a new derivative in the B folder where they are given sequential file names based on the parent item name. The code is currently set up to give sequential files names by the position of each photograph in a clockwise rotation, but this can be configured to whatever you find intuitive. &#10042;"
    },
    { 
        "url": "/vra_2025/content/4_findings.html",
        "title": "Findings",
        "text": "edge_detector successful applications Despite the Edge Detection tool working with seemingly more complex materials in archeological items with intricate edges, while the Image Extraction tool is generally working with rectangles, the latter is much less accurate. I think this is due to coincidental similarities of archeological photographs and the commercial photography that IS-NET was developed for. Both use fairly new cameras at close range and identify and extract a single, principle object in each image. The model is extensively trained because, unsurprisingly, there is incredible economic interest in streamlining the digital processing of commercial products and it just happens to overlap with this academic and preservation oriented discipline. &#10042; edge_detector unsuccessful examples On the 24 objects I tested with this tool, including some tricky, nearly transparent glass items, the tool only produced three minor inaccuracies: Even though the code specifies only finding a single, principle object in each image, the tool successfully extracts both shards of nearly transparent glass in the bottom image, but it also includes a very slight shadow, and the ruler next to this pencil is being lumped into the extracted image, possibly due to proximity. Additionally, there is one object you can see on the banner for this presentation that I will return to, where some of the text within the paper wrapper has been misidentified as it’s black background. &#10042; image_extractor successful example In this example of a scrapbook page with a plain background and fairly straight, somewhat evenly spaced photos, it identified and correctly extracted 25 of the 26 images, although there are still minor inaccuracies around the margins of the photographs, not fully rotating the images to 90 degree angles. &#10042; image_extractor least successful examples In these two examples where photographs within the scrapbook page are skewed, overlapping, touching the borders of the full image, on paper containing designs or containing multiple background colors, the success rate of extraction was around 73 percent. At that level of accuracy, you might be better off approaching these projects manually. Despite seeming more geometrically straight forward, extracting photographs within scrapbooks involves a deceptive amount of variance. The shapes of photographs may be rectangular, circular or with scalloped edges. The background paper may be white, beige or black. Additionally, if a photograph is extracted successfully at a right angle, the image itself may be skewed within the border. All of this, along with the complication of identifying multiple objects, which allows for the possibility of false positives in the form of noise within the page, made this a more complicated task than I would have assumed at the outset. It’s no wonder the edge detector, at a slim 45 lines of code and built around a single, extensively trained model, performs more accurately. The image extractor is twice as long and relies on multiple machine learning libraries to resolve the many potential conflicts which may or may not be involved in it’s task. &#10042; Conclusion My biggest take away from working on these tools is to seek out models working in parallel disciplines which may be much more well funded than the archives. I “developed” these tools so far as I implemented an infrastructure for different machine learning and neural network models to produce outputs in the format that best serves our preservation needs at U of I, but the development of those models took so much more time and technical expertise than I will ever have as a librarian. That said, if we can identify these these parallel applications in better funded commercial spaces that have analogues to our materials, we can leverage them to create open source, scalable tools for digital processing to increase the efficiency of our archival practices. About the Author Andrew Weymouth is the Digital Initiatives Librarian at University of Idaho, primarily focusing on static web design to curate the institution’s special collections and partner with faculty and graduate students on fellowship projects. He has also created digital scholarship projects for the universities of Oregon, Washington and the Tacoma Northwest Room archives, ranging from long form audio public history to architectural databases and network visualizations. He writes about labor, architecture, underrepresented communities and using digital scholarship methods to survey equity in archival collections. More Workshops from the Author"
    },
    { 
        "url": "/vra_2025/",
        "title": "Home",
        "text": "Image Extraction for Overhead Scans and Archeological Photographs slide deck git for edge_detector git for image_extractor Presentation for the 2025 Visual Resources Association Conference. Contents: Introduction edge_detector tool image_extractor tool Findings Content: CC BY-NC-ND 4.0 Andrew Weymouth 2025 (get source code). Theme: Variation on workshop-template-b by evanwill"
    }];
